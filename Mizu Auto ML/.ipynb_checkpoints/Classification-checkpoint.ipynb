{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.stats import chisquare, chi2_contingency\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "import os\n",
    "import gc\n",
    "import shap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath= 'UCI_Credit_Card.csv'\n",
    "def get_type(datapath):\n",
    "    extension = datapath.split('.')[1]\n",
    "    assert datapath.endswith(tuple(['xls', 'xlsx', 'csv'])), 'Our system currently only accepts csv, xls or xlsx extensions, your input was {}'.format(extension)\n",
    "    if 'csv' in datapath:\n",
    "        seplist = [',', '|', ';', '\\t']\n",
    "        return seplist\n",
    "    elif 'xls'in datapath or 'xlsx' in datapath:\n",
    "        xl = pd.ExcelFile(datapath)\n",
    "        return xl.sheet_names\n",
    "    else:\n",
    "        print('Our system currently only accepts csv, xls or xlsx extensions')\n",
    "\n",
    "def read_data(datapath, select):\n",
    "    extension = datapath.split('.')[1]\n",
    "    assert datapath.endswith(tuple(['xls', 'xlsx', 'csv'])), 'Our system currently only accepts csv, xls or xlsx extensions, your input was {}'.format(extension)\n",
    "    if 'csv' in datapath:\n",
    "        return pd.read_csv(datapath, sep=select)\n",
    "    elif 'xls'in datapath or 'xlsx' in datapath:\n",
    "        return pd.read_excel(datapath, sheet=select)\n",
    "    else:\n",
    "        print('Our system currently only accepts csv, xls or xlsx extensions')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(datapath, get_type(datapath)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data)\n",
    "target = 'default.payment.next.month'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data was dropped from 25 to 24\n"
     ]
    }
   ],
   "source": [
    "## Remove IDs and single values\n",
    "a = data.shape[1]\n",
    "for col in list(data):\n",
    "    if data[col].nunique() ==1:\n",
    "        data.drop(columns=[col], inplace=True)\n",
    "    elif data[col].nunique() == len(data):\n",
    "        data.drop(columns=[col], inplace=True)\n",
    "    else:\n",
    "        None\n",
    "\n",
    "b = data.shape[1]\n",
    "print(\"data was dropped from {} to {}\".format(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "addition = [np.nan] * (data.shape[1]-1)\n",
    "additional = pd.concat([pd.DataFrame(addition), pd.DataFrame([0])], ignore_index=True).T\n",
    "additional.columns = data.columns.values\n",
    "\n",
    "data = pd.concat([data, additional], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>150000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8979.0</td>\n",
       "      <td>5190.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1837.0</td>\n",
       "      <td>3526.0</td>\n",
       "      <td>8998.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20878.0</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>19357.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>80000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52774.0</td>\n",
       "      <td>11855.0</td>\n",
       "      <td>48944.0</td>\n",
       "      <td>85900.0</td>\n",
       "      <td>3409.0</td>\n",
       "      <td>1178.0</td>\n",
       "      <td>1926.0</td>\n",
       "      <td>52964.0</td>\n",
       "      <td>1804.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36535.0</td>\n",
       "      <td>32428.0</td>\n",
       "      <td>15313.0</td>\n",
       "      <td>2078.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30001 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LIMIT_BAL  SEX  EDUCATION  MARRIAGE   AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0        20000.0  2.0        2.0       1.0  24.0    2.0    2.0   -1.0   -1.0   \n",
       "1       120000.0  2.0        2.0       2.0  26.0   -1.0    2.0    0.0    0.0   \n",
       "2        90000.0  2.0        2.0       2.0  34.0    0.0    0.0    0.0    0.0   \n",
       "3        50000.0  2.0        2.0       1.0  37.0    0.0    0.0    0.0    0.0   \n",
       "4        50000.0  1.0        2.0       1.0  57.0   -1.0    0.0   -1.0    0.0   \n",
       "...          ...  ...        ...       ...   ...    ...    ...    ...    ...   \n",
       "29996   150000.0  1.0        3.0       2.0  43.0   -1.0   -1.0   -1.0   -1.0   \n",
       "29997    30000.0  1.0        2.0       2.0  37.0    4.0    3.0    2.0   -1.0   \n",
       "29998    80000.0  1.0        3.0       1.0  41.0    1.0   -1.0    0.0    0.0   \n",
       "29999    50000.0  1.0        2.0       1.0  46.0    0.0    0.0    0.0    0.0   \n",
       "30000        NaN  NaN        NaN       NaN   NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "       PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
       "0       -2.0  ...        0.0        0.0        0.0       0.0     689.0   \n",
       "1        0.0  ...     3272.0     3455.0     3261.0       0.0    1000.0   \n",
       "2        0.0  ...    14331.0    14948.0    15549.0    1518.0    1500.0   \n",
       "3        0.0  ...    28314.0    28959.0    29547.0    2000.0    2019.0   \n",
       "4        0.0  ...    20940.0    19146.0    19131.0    2000.0   36681.0   \n",
       "...      ...  ...        ...        ...        ...       ...       ...   \n",
       "29996    0.0  ...     8979.0     5190.0        0.0    1837.0    3526.0   \n",
       "29997    0.0  ...    20878.0    20582.0    19357.0       0.0       0.0   \n",
       "29998    0.0  ...    52774.0    11855.0    48944.0   85900.0    3409.0   \n",
       "29999    0.0  ...    36535.0    32428.0    15313.0    2078.0    1800.0   \n",
       "30000    NaN  ...        NaN        NaN        NaN       NaN       NaN   \n",
       "\n",
       "       PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0           0.0       0.0       0.0       0.0                         1.0  \n",
       "1        1000.0    1000.0       0.0    2000.0                         1.0  \n",
       "2        1000.0    1000.0    1000.0    5000.0                         0.0  \n",
       "3        1200.0    1100.0    1069.0    1000.0                         0.0  \n",
       "4       10000.0    9000.0     689.0     679.0                         0.0  \n",
       "...         ...       ...       ...       ...                         ...  \n",
       "29996    8998.0     129.0       0.0       0.0                         0.0  \n",
       "29997   22000.0    4200.0    2000.0    3100.0                         1.0  \n",
       "29998    1178.0    1926.0   52964.0    1804.0                         1.0  \n",
       "29999    1430.0    1000.0    1000.0    1000.0                         1.0  \n",
       "30000       NaN       NaN       NaN       NaN                         0.0  \n",
       "\n",
       "[30001 rows x 24 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=1, thresh=int(np.ceil(0.2*len(data))))\n",
    "y = data[target]\n",
    "x = data.drop([target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_use = list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_ob(x):\n",
    "    x_num = x.select_dtypes(exclude='object')\n",
    "    x_ob = x.select_dtypes(include='object')\n",
    "    return x_num, x_ob\n",
    "def imput_fit_transform(x):\n",
    "    imput = SimpleImputer(strategy='median')\n",
    "    x_numeric_imp = pd.DataFrame(imput.fit_transform(x), columns = x.columns, index = x.index)\n",
    "    return imput, x_numeric_imp\n",
    "\n",
    "def imput_transform(x, imput):\n",
    "    x_numeric_imp = pd.DataFrame(imput.transform(x), columns = x.columns, index = x.index)\n",
    "    return x_numeric_imp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xnum, xobj = get_num_ob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(xobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[]"
      ],
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "d = defaultdict(LabelEncoder)\n",
    "\n",
    "xobj.fillna('Unknown', inplace=True)\n",
    "\n",
    "## get labeled\n",
    "def le_fit_transform(df):\n",
    "    le = LabelEncoder()\n",
    "    fit = df.apply(lambda x: d[x.name].fit_transform(x))\n",
    "    return fit, d\n",
    "def le_transform(df, le):\n",
    "    x_transformed = df.apply(lambda x: le[x.name].transform(x))\n",
    "    return x_transformed\n",
    "\n",
    "x_obj_le, le = le_fit_transform(xobj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_con = pd.concat([xnum, x_obj_le], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_con = x_con.dropna(axis=1, thresh=int(np.ceil(0.2*len(x_con))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "droppage = len(x_con)-1\n",
    "x_con.drop(droppage, inplace=True)\n",
    "y.drop(droppage, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_con, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_obj = x_train[categories]\n",
    "x_train_num = x_train[set(list(x_train)) - set(categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multicol_filter(df, min_v, max_v):\n",
    "    columnss = np.full((df.shape[0],), True, dtype=bool)\n",
    "    for i in range(df.shape[0]):\n",
    "        for j in range(i+1, df.shape[0]):\n",
    "            if df.iloc[i,j] >=max_v or df.iloc[i,j] <=min_v:\n",
    "                if columnss[j]:\n",
    "                    columnss[j] = False\n",
    "\n",
    "    ss = df.head(1)\n",
    "    selected_columnss = ss.columns[columnss]\n",
    "    ts = list(ss[selected_columnss])\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 23) (24000, 17)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ze = x_train_num.corr()\n",
    "t = multicol_filter(ze, -0.8, 0.8)\n",
    "\n",
    "xnum_clean = x_train_num[t]\n",
    "print(x_train_num.shape, xnum_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cramers_V(var1,var2) :\n",
    "    crosstab =np.array(pd.crosstab(var1,var2, rownames=None, colnames=None)) # Cross table building\n",
    "    stat = chi2_contingency(crosstab)[0] # Keeping of the test statistic of the Chi2 test\n",
    "    obs = np.sum(crosstab) # Number of observations\n",
    "    mini = min(crosstab.shape)-1 # Take the minimum value between the columns and the rows of the cross table\n",
    "    return (stat/(obs*mini))\n",
    "\n",
    "def chi_test(data, categories):\n",
    "    drop_cols = []\n",
    "    for j in range(len(categories)-1):\n",
    "            for k in range(j+1, len(categories)):\n",
    "\n",
    "                pvalue = chi2_contingency(pd.crosstab(data[categories[j]],data[categories[k]]))[1]\n",
    "                if pvalue < 0.05:\n",
    "                    if categories[k] in drop_cols:\n",
    "                        None\n",
    "                    else:\n",
    "                        drop_cols.append(categories[k])\n",
    "                else:\n",
    "                    None\n",
    "    return set(categories) - set(np.unique(drop_cols))\n",
    "\n",
    "def cramer_test(data, max_v):\n",
    "    rows= []\n",
    "    for var1 in data:\n",
    "        col = []\n",
    "        for var2 in data :\n",
    "            cramers =cramers_V(data[var1], data[var2]) # Cramer's V test\n",
    "            col.append(round(cramers,2)) # Keeping of the rounded value of the Cramer's V  \n",
    "        rows.append(col)\n",
    "\n",
    "    cramers_results = np.array(rows)\n",
    "    df = pd.DataFrame(cramers_results, columns = data.columns, index =data.columns)\n",
    "    return multicol_filter(df, -max_v, max_v)\n",
    "def categorical_filter(data, max_v, method = 'chi2'):\n",
    "    '''\n",
    "    filter the categorical features using either cramers v, chi squared or intersection of both\n",
    "    data = dataframe to add\n",
    "    max_v = used for the common benchmark of colinearity value\n",
    "    method = string input accepting either 'chi2', 'cramer', or 'both'\n",
    "    '''\n",
    "    assert method in ['chi2', 'cramer', 'both'], 'method not understandable, please use either chi2, cramer or both'\n",
    "    categories = list(data)\n",
    "    \n",
    "    if method == 'chi2':\n",
    "        keep_cols = chi_test(data, categories)\n",
    "        \n",
    "    elif method == 'cramer':\n",
    "        keep_cols = cramer_test(data, max_v)\n",
    "    \n",
    "    elif method == 'both':\n",
    "        keep_cols_chi = chi_test(data, categories)\n",
    "        keep_cols_cv = cramer_test(data, max_v)\n",
    "        del_chi = set(categories) - set(keep_cols_chi)\n",
    "        del_cv = set(categories) - set(keep_cols_cv)\n",
    "        del_both = set(del_chi).intersection(set(del_cv))\n",
    "        keep_cols = set(categories) -  set(del_both)\n",
    "    else:\n",
    "        print(\"error with method\")\n",
    "    \n",
    "    return keep_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    filterer = categorical_filter(x_train_obj, 0.8, method = 'both')\n",
    "except:\n",
    "    filterer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_obj_clean = x_train_obj[filterer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat back to x_train\n",
    "x_train_clean = pd.concat([xnum_clean, x_obj_clean], axis=1)\n",
    "x_test = x_test[x_train_clean.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer, x_train_imputed = imput_fit_transform(x_train_clean)\n",
    "x_test_imputed = imput_transform(x_test, imputer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=108)\n",
    "tl = TomekLinks()\n",
    "x_train_ov, y_train_ov = sm.fit_resample(x_train_imputed, y_train)\n",
    "x_train_un, y_train_un = tl.fit_resample(x_train_imputed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=108)\n",
    "rf = RandomForestClassifier(random_state=108)\n",
    "gb = GradientBoostingClassifier(random_state=108)\n",
    "cb = CatBoostClassifier(random_state=108, verbose=False)\n",
    "dt_param = {'max_depth':[1, 3, 5, 10], 'min_samples_split':[2,4,8,16], 'min_samples_leaf':[1,2,4,6,8,10]}\n",
    "\n",
    "\n",
    "n_estimators = [10, 25, 50, 100]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [3, 5, 10, 12, None]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "random_strength = [0.0001, 0.001, 0.1, 1]\n",
    "border_count = [1, 5, 10, 25, 50, 100, 255]\n",
    "l2_leaf_reg = [1, 2, 3, 4, 5, 6, 10, 15, 30]\n",
    "bagging_temperature = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "rf_param = {'n_estimators': n_estimators, 'max_features':max_features, 'max_depth':max_depth, 'min_samples_split':min_samples_split,'min_samples_leaf':min_samples_leaf}\n",
    "\n",
    "learning_rates = [1, 0.5, 0.25, 0.1, 0.05, 0.01]\n",
    "gb_param = {'learning_rate':learning_rates, 'n_estimators': n_estimators, 'max_depth':max_depth, 'min_samples_split':min_samples_split,'min_samples_leaf':min_samples_leaf, 'max_features':max_features}\n",
    "cb_param = {'learning_rate':learning_rates, 'iterations': n_estimators, 'depth':max_depth, 'random_strength':random_strength,'border_count':border_count, 'l2_leaf_reg':l2_leaf_reg, 'bagging_temperature':bagging_temperature}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algo</th>\n",
       "      <th>n_features</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>method</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gb</td>\n",
       "      <td>25</td>\n",
       "      <td>0.783913</td>\n",
       "      <td>0.773701</td>\n",
       "      <td>undersampling</td>\n",
       "      <td>[PAY_0, PAY_2, PAY_3, BILL_AMT4, PAY_4, LIMIT_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  algo  n_features  train_auc  test_auc         method  \\\n",
       "6   gb          25   0.783913  0.773701  undersampling   \n",
       "\n",
       "                                            features  \n",
       "6  [PAY_0, PAY_2, PAY_3, BILL_AMT4, PAY_4, LIMIT_...  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name = []\n",
    "k = []\n",
    "tr_auc = []\n",
    "te_auc = []\n",
    "method = []\n",
    "features = []\n",
    "trans = dict()\n",
    "for data_used in [[x_train_ov, y_train_ov, 'oversampling'], [x_train_un, y_train_un, 'undersampling']]:\n",
    "    x_use = data_used[0]\n",
    "    y_use = data_used[1]\n",
    "    gdt = RandomizedSearchCV(dt, dt_param, n_jobs=-1, scoring='roc_auc', n_iter=10, random_state=108)\n",
    "    grf = RandomizedSearchCV(rf, rf_param, n_jobs=-1, scoring='roc_auc', n_iter=10, random_state=108)\n",
    "    ggb = RandomizedSearchCV(gb, gb_param, n_jobs=-1, scoring='roc_auc', n_iter=10, random_state=108)\n",
    "    gcb = RandomizedSearchCV(cb, cb_param, n_jobs=-1, scoring='roc_auc', n_iter=20, random_state=108)\n",
    "    new_dt = DecisionTreeClassifier(**gdt.fit(x_use, y_use).best_params_, random_state=108)\n",
    "    \n",
    "    new_rf = RandomForestClassifier(**grf.fit(x_use, y_use).best_params_, random_state=108)\n",
    "    \n",
    "    new_gb = GradientBoostingClassifier(**ggb.fit(x_use, y_use).best_params_, random_state=108)\n",
    "    \n",
    "    new_cb = CatBoostClassifier(**gcb.fit(x_use, y_use).best_params_, random_state=108, verbose=False)\n",
    "\n",
    "\n",
    "    for algo in [[new_dt, 'dt'], [new_rf, 'rf'], [new_gb, 'gb'], [new_cb, 'cb']]:\n",
    "        algo[0].fit(x_use, y_use)\n",
    "        current = 0\n",
    "        num = x_train_imputed.shape[1]\n",
    "        used_feature = list(x_use)\n",
    "        sampling = 'normal'\n",
    "        usee = pd.DataFrame({'params':x_use.columns, 'importances':algo[0].feature_importances_}).sort_values('importances', ascending=False)\n",
    "        for kbest in [5, 10, 15, 25, 50]:\n",
    "            uses = usee.head(kbest)['params']\n",
    "            \n",
    "\n",
    "            x_tr_try= x_use[uses]\n",
    "            \n",
    "            hold = np.mean(cross_val_score(estimator=algo[0], X=x_tr_try, y=y_use, cv = 5, scoring = 'roc_auc'))\n",
    "            if hold > current:\n",
    "                current = hold\n",
    "                num = kbest       \n",
    "                sampling = data_used[2]\n",
    "                used_feature = list(uses)\n",
    "            else:\n",
    "                None\n",
    "\n",
    "        x_tr_fin = x_use[usee.head(num)['params']]\n",
    "        x_te_fin = x_test_imputed[usee.head(num)['params']]\n",
    "        \n",
    "        y_pred = algo[0].fit(x_tr_fin, y_use).predict_proba(x_te_fin)\n",
    "        store = roc_auc_score(y_test, y_pred[:,1])\n",
    "        \n",
    "        name.append(algo[1])\n",
    "        k.append(num)\n",
    "        tr_auc.append(current)\n",
    "        te_auc.append(store)\n",
    "        method.append(sampling)\n",
    "        features.append(used_feature)\n",
    "\n",
    "result = pd.DataFrame({'algo':name, 'n_features':k, 'train_auc':tr_auc, 'test_auc':te_auc, 'method':method, 'features':features}).sort_values('test_auc', ascending=False)\n",
    "result.sort_values('test_auc', ascending=False).head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algo</th>\n",
       "      <th>n_features</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>method</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gb</td>\n",
       "      <td>25</td>\n",
       "      <td>0.783913</td>\n",
       "      <td>0.773701</td>\n",
       "      <td>undersampling</td>\n",
       "      <td>[PAY_0, PAY_2, PAY_3, BILL_AMT4, PAY_4, LIMIT_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf</td>\n",
       "      <td>15</td>\n",
       "      <td>0.786314</td>\n",
       "      <td>0.772733</td>\n",
       "      <td>undersampling</td>\n",
       "      <td>[PAY_0, PAY_2, PAY_3, BILL_AMT4, LIMIT_BAL, PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cb</td>\n",
       "      <td>25</td>\n",
       "      <td>0.783223</td>\n",
       "      <td>0.771076</td>\n",
       "      <td>undersampling</td>\n",
       "      <td>[PAY_0, LIMIT_BAL, PAY_AMT1, PAY_3, PAY_2, PAY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf</td>\n",
       "      <td>25</td>\n",
       "      <td>0.938444</td>\n",
       "      <td>0.760515</td>\n",
       "      <td>oversampling</td>\n",
       "      <td>[PAY_0, MARRIAGE, PAY_2, PAY_3, EDUCATION, SEX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cb</td>\n",
       "      <td>15</td>\n",
       "      <td>0.928123</td>\n",
       "      <td>0.758531</td>\n",
       "      <td>oversampling</td>\n",
       "      <td>[SEX, EDUCATION, MARRIAGE, PAY_0, PAY_6, LIMIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dt</td>\n",
       "      <td>5</td>\n",
       "      <td>0.759917</td>\n",
       "      <td>0.751347</td>\n",
       "      <td>undersampling</td>\n",
       "      <td>[PAY_0, PAY_2, PAY_AMT3, LIMIT_BAL, PAY_6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dt</td>\n",
       "      <td>15</td>\n",
       "      <td>0.864892</td>\n",
       "      <td>0.746982</td>\n",
       "      <td>oversampling</td>\n",
       "      <td>[PAY_0, PAY_2, PAY_4, PAY_AMT1, PAY_6, LIMIT_B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gb</td>\n",
       "      <td>15</td>\n",
       "      <td>0.938421</td>\n",
       "      <td>0.740443</td>\n",
       "      <td>oversampling</td>\n",
       "      <td>[PAY_0, PAY_2, MARRIAGE, SEX, PAY_3, BILL_AMT4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  algo  n_features  train_auc  test_auc         method  \\\n",
       "6   gb          25   0.783913  0.773701  undersampling   \n",
       "5   rf          15   0.786314  0.772733  undersampling   \n",
       "7   cb          25   0.783223  0.771076  undersampling   \n",
       "1   rf          25   0.938444  0.760515   oversampling   \n",
       "3   cb          15   0.928123  0.758531   oversampling   \n",
       "4   dt           5   0.759917  0.751347  undersampling   \n",
       "0   dt          15   0.864892  0.746982   oversampling   \n",
       "2   gb          15   0.938421  0.740443   oversampling   \n",
       "\n",
       "                                            features  \n",
       "6  [PAY_0, PAY_2, PAY_3, BILL_AMT4, PAY_4, LIMIT_...  \n",
       "5  [PAY_0, PAY_2, PAY_3, BILL_AMT4, LIMIT_BAL, PA...  \n",
       "7  [PAY_0, LIMIT_BAL, PAY_AMT1, PAY_3, PAY_2, PAY...  \n",
       "1  [PAY_0, MARRIAGE, PAY_2, PAY_3, EDUCATION, SEX...  \n",
       "3  [SEX, EDUCATION, MARRIAGE, PAY_0, PAY_6, LIMIT...  \n",
       "4         [PAY_0, PAY_2, PAY_AMT3, LIMIT_BAL, PAY_6]  \n",
       "0  [PAY_0, PAY_2, PAY_4, PAY_AMT1, PAY_6, LIMIT_B...  \n",
       "2  [PAY_0, PAY_2, MARRIAGE, SEX, PAY_3, BILL_AMT4...  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_used = result['algo'].iloc[0]\n",
    "features_used = result['features'].iloc[0]\n",
    "sampling_used = result['method'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algo_used == 'dt':\n",
    "    do_train = new_dt\n",
    "elif algo_used == 'gb':\n",
    "    do_train = new_gb\n",
    "elif algo_used == 'rf':\n",
    "    do_train = new_rf\n",
    "elif algo_used == 'cb':\n",
    "    do_train = new_cb\n",
    "    \n",
    "if sampling_used == 'undersampling':\n",
    "    do_sampling = TomekLinks()\n",
    "elif sampling_used == 'oversampling':\n",
    "    do_sampling = SMOTE(random_state=108)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare to retrain using all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we happened to already do our part in the x_con, so we will reuse x_con as our main retraining dataset.\n",
    "# Since we already do label encoding, we no longer need to label encode it again\n",
    "imputer, x_imputed = imput_fit_transform(x_con)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only the best features from train_test_split\n",
    "x_imputed_use = x_imputed[features_used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 17)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_imputed_use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sampled_use, y_sampled = do_sampling.fit_resample(x_imputed_use, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.25, loss='deviance', max_depth=5,\n",
       "                           max_features='sqrt', max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=4, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=108, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_train.fit(x_sampled_use, y_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8281781641106357"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y,  do_train.predict_proba(x_imputed_use)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_datapath= 'train.csv'\n",
    "pred_data = read_data(datapath, get_type(datapath)[0])\n",
    "pred_data = pred_data[list(x_con)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "pred_data_obj = pred_data[categories]\n",
    "pred_data_obj.fillna('Unknown', inplace=True)\n",
    "pred_data_num = pred_data[set(pred_data) - set(categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_obj_le = le_transform(pred_data_obj, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_con = pd.concat([pred_data_num, pred_data_obj_le], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_con = pred_data_con[list(x_con)]\n",
    "pred_con_imputed = imput_transform(pred_data_con, imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_con_use = pred_con_imputed[features_used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data['prediction_result'] = do_train.predict_proba(pred_con_use)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
