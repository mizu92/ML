{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import chisquare, chi2_contingency\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import gc\n",
    "import shap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath= 'train.csv'\n",
    "def get_type(datapath):\n",
    "    extension = datapath.split('.')[1]\n",
    "    assert datapath.endswith(tuple(['xls', 'xlsx', 'csv'])), 'Our system currently only accepts csv, xls or xlsx extensions, your input was {}'.format(extension)\n",
    "    if 'csv' in datapath:\n",
    "        seplist = [',', '|', ';', '\\t']\n",
    "        return seplist\n",
    "    elif 'xls'in datapath or 'xlsx' in datapath:\n",
    "        xl = pd.ExcelFile(datapath)\n",
    "        return xl.sheet_names\n",
    "    else:\n",
    "        print('Our system currently only accepts csv, xls or xlsx extensions')\n",
    "\n",
    "def read_data(datapath, select):\n",
    "    extension = datapath.split('.')[1]\n",
    "    assert datapath.endswith(tuple(['xls', 'xlsx', 'csv'])), 'Our system currently only accepts csv, xls or xlsx extensions, your input was {}'.format(extension)\n",
    "    if 'csv' in datapath:\n",
    "        return pd.read_csv(datapath, sep=select)\n",
    "    elif 'xls'in datapath or 'xlsx' in datapath:\n",
    "        return pd.read_excel(datapath, sheet=select)\n",
    "    else:\n",
    "        print('Our system currently only accepts csv, xls or xlsx extensions')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(datapath, get_type(datapath)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data)\n",
    "target = 'SalePrice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data was dropped from 81 to 80\n"
     ]
    }
   ],
   "source": [
    "## Remove IDs and single values\n",
    "a = data.shape[1]\n",
    "for col in list(data):\n",
    "    if data[col].nunique() ==1:\n",
    "        data.drop(columns=[col], inplace=True)\n",
    "    elif data[col].nunique() == len(data):\n",
    "        data.drop(columns=[col], inplace=True)\n",
    "    else:\n",
    "        None\n",
    "\n",
    "b = data.shape[1]\n",
    "print(\"data was dropped from {} to {}\".format(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "addition = [np.nan] * (data.shape[1]-1)\n",
    "additional = pd.concat([pd.DataFrame(addition), pd.DataFrame([0])], ignore_index=True).T\n",
    "additional.columns = data.columns.values\n",
    "\n",
    "data = pd.concat([data, additional], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=1, thresh=int(np.ceil(0.2*len(data))))\n",
    "y = data[target]\n",
    "x = data.drop([target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_use = list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_ob(x):\n",
    "    x_num = x.select_dtypes(exclude='object')\n",
    "    x_ob = x.select_dtypes(include='object')\n",
    "    return x_num, x_ob\n",
    "def imput_fit_transform(x):\n",
    "    imput = SimpleImputer(strategy='median')\n",
    "    x_numeric_imp = pd.DataFrame(imput.fit_transform(x), columns = x.columns, index = x.index)\n",
    "    return imput, x_numeric_imp\n",
    "\n",
    "def imput_transform(x, imput):\n",
    "    x_numeric_imp = pd.DataFrame(imput.transform(x), columns = x.columns, index = x.index)\n",
    "    return x_numeric_imp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xnum, xobj = get_num_ob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(xobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "d = defaultdict(LabelEncoder)\n",
    "\n",
    "xobj.fillna('Unknown', inplace=True)\n",
    "\n",
    "## get labeled\n",
    "def le_fit_transform(df):\n",
    "    le = LabelEncoder()\n",
    "    fit = df.apply(lambda x: d[x.name].fit_transform(x))\n",
    "    return fit, d\n",
    "def le_transform(df, le):\n",
    "    x_transformed = df.apply(lambda x: le[x.name].transform(x))\n",
    "    return x_transformed\n",
    "\n",
    "x_obj_le, le = le_fit_transform(xobj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_con = pd.concat([xnum, x_obj_le], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "droppage = len(x_con)-1\n",
    "x_con.drop(droppage, inplace=True)\n",
    "y.drop(droppage, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_con, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_obj = x_train[categories]\n",
    "x_train_num = x_train[set(list(x_train)) - set(categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multicol_filter(df, min_v, max_v):\n",
    "    columnss = np.full((df.shape[0],), True, dtype=bool)\n",
    "    for i in range(df.shape[0]):\n",
    "        for j in range(i+1, df.shape[0]):\n",
    "            if df.iloc[i,j] >=max_v or df.iloc[i,j] <=min_v:\n",
    "                if columnss[j]:\n",
    "                    columnss[j] = False\n",
    "\n",
    "    ss = df.head(1)\n",
    "    selected_columnss = ss.columns[columnss]\n",
    "    ts = list(ss[selected_columnss])\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 36) (1168, 32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ze = x_train_num.corr()\n",
    "t = multicol_filter(ze, -0.8, 0.8)\n",
    "\n",
    "xnum_clean = x_train_num[t]\n",
    "print(x_train_num.shape, xnum_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cramers_V(var1,var2) :\n",
    "    crosstab =np.array(pd.crosstab(var1,var2, rownames=None, colnames=None)) # Cross table building\n",
    "    stat = chi2_contingency(crosstab)[0] # Keeping of the test statistic of the Chi2 test\n",
    "    obs = np.sum(crosstab) # Number of observations\n",
    "    mini = min(crosstab.shape)-1 # Take the minimum value between the columns and the rows of the cross table\n",
    "    return (stat/(obs*mini))\n",
    "\n",
    "def chi_test(data, categories):\n",
    "    drop_cols = []\n",
    "    for j in range(len(categories)-1):\n",
    "            for k in range(j+1, len(categories)):\n",
    "\n",
    "                pvalue = chi2_contingency(pd.crosstab(data[categories[j]],data[categories[k]]))[1]\n",
    "                if pvalue < 0.05:\n",
    "                    if categories[k] in drop_cols:\n",
    "                        None\n",
    "                    else:\n",
    "                        drop_cols.append(categories[k])\n",
    "                else:\n",
    "                    None\n",
    "    return set(categories) - set(np.unique(drop_cols))\n",
    "\n",
    "def cramer_test(data, max_v):\n",
    "    rows= []\n",
    "    for var1 in data:\n",
    "        col = []\n",
    "        for var2 in data :\n",
    "            cramers =cramers_V(data[var1], data[var2]) # Cramer's V test\n",
    "            col.append(round(cramers,2)) # Keeping of the rounded value of the Cramer's V  \n",
    "        rows.append(col)\n",
    "\n",
    "    cramers_results = np.array(rows)\n",
    "    df = pd.DataFrame(cramers_results, columns = data.columns, index =data.columns)\n",
    "    return multicol_filter(df, -max_v, max_v)\n",
    "def categorical_filter(data, max_v, method = 'chi2'):\n",
    "    '''\n",
    "    filter the categorical features using either cramers v, chi squared or intersection of both\n",
    "    data = dataframe to add\n",
    "    max_v = used for the common benchmark of colinearity value\n",
    "    method = string input accepting either 'chi2', 'cramer', or 'both'\n",
    "    '''\n",
    "    assert method in ['chi2', 'cramer', 'both'], 'method not understandable, please use either chi2, cramer or both'\n",
    "    categories = list(data)\n",
    "    \n",
    "    if method == 'chi2':\n",
    "        keep_cols = chi_test(data, categories)\n",
    "        \n",
    "    elif method == 'cramer':\n",
    "        keep_cols = cramer_test(data, max_v)\n",
    "    \n",
    "    elif method == 'both':\n",
    "        keep_cols_chi = chi_test(data, categories)\n",
    "        keep_cols_cv = cramer_test(data, max_v)\n",
    "        del_chi = set(categories) - set(keep_cols_chi)\n",
    "        del_cv = set(categories) - set(keep_cols_cv)\n",
    "        del_both = set(del_chi).intersection(set(del_cv))\n",
    "        keep_cols = set(categories) -  set(del_both)\n",
    "    else:\n",
    "        print(\"error with method\")\n",
    "    \n",
    "    return keep_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterer = categorical_filter(x_train_obj, 0.8, method = 'both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_obj_clean = x_train_obj[filterer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat back to x_train\n",
    "x_train_clean = pd.concat([xnum_clean, x_obj_clean], axis=1)\n",
    "x_test = x_test[x_train_clean.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer, x_train_imputed = imput_fit_transform(x_train_clean)\n",
    "x_test_imputed = imput_transform(x_test, imputer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(random_state=108)\n",
    "rf = RandomForestRegressor(random_state=108)\n",
    "gb = GradientBoostingRegressor(random_state=108)\n",
    "cb = CatBoostRegressor(random_state=108, verbose=False)\n",
    "dt_param = {'max_depth':[1, 3, 5, 10], 'min_samples_split':[2,4,8,16], 'min_samples_leaf':[1,2,4,6,8,10]}\n",
    "\n",
    "\n",
    "n_estimators = [10, 25, 50, 100]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [3, 5, 10, 12, None]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "random_strength = [0.0001, 0.001, 0.1, 1]\n",
    "border_count = [1, 5, 10, 25, 50, 100, 255]\n",
    "l2_leaf_reg = [1, 2, 3, 4, 5, 6, 10, 15, 30]\n",
    "bagging_temperature = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "rf_param = {'n_estimators': n_estimators, 'max_features':max_features, 'max_depth':max_depth, 'min_samples_split':min_samples_split,'min_samples_leaf':min_samples_leaf}\n",
    "\n",
    "learning_rates = [1, 0.5, 0.25, 0.1, 0.05, 0.01]\n",
    "gb_param = {'learning_rate':learning_rates, 'n_estimators': n_estimators, 'max_depth':max_depth, 'min_samples_split':min_samples_split,'min_samples_leaf':min_samples_leaf, 'max_features':max_features}\n",
    "cb_param = {'learning_rate':learning_rates, 'iterations': n_estimators, 'depth':max_depth, 'random_strength':random_strength,'border_count':border_count, 'l2_leaf_reg':l2_leaf_reg, 'bagging_temperature':bagging_temperature}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algo</th>\n",
       "      <th>n_features</th>\n",
       "      <th>train_RMSE</th>\n",
       "      <th>test_RMSE</th>\n",
       "      <th>method</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gb</td>\n",
       "      <td>15</td>\n",
       "      <td>29685.056366</td>\n",
       "      <td>29492.666642</td>\n",
       "      <td>normal</td>\n",
       "      <td>[OverallQual, GrLivArea, 2ndFlrSF, TotalBsmtSF...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  algo  n_features    train_RMSE     test_RMSE  method  \\\n",
       "2   gb          15  29685.056366  29492.666642  normal   \n",
       "\n",
       "                                            features  \n",
       "2  [OverallQual, GrLivArea, 2ndFlrSF, TotalBsmtSF...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name = []\n",
    "k = []\n",
    "tr_auc = []\n",
    "te_auc = []\n",
    "method = []\n",
    "features = []\n",
    "trans = dict()\n",
    "for data_used in [[x_train_imputed, y_train, 'normal']]:\n",
    "    x_use = data_used[0]\n",
    "    y_use = data_used[1]\n",
    "    gdt = RandomizedSearchCV(dt, dt_param, n_jobs=-1, scoring='neg_root_mean_squared_error', n_iter=10, random_state=108)\n",
    "    grf = RandomizedSearchCV(rf, rf_param, n_jobs=-1, scoring='neg_root_mean_squared_error', n_iter=10, random_state=108)\n",
    "    ggb = RandomizedSearchCV(gb, gb_param, n_jobs=-1, scoring='neg_root_mean_squared_error', n_iter=10, random_state=108)\n",
    "    gcb = RandomizedSearchCV(cb, cb_param, n_jobs=-1, scoring='neg_root_mean_squared_error', n_iter=20, random_state=108)\n",
    "    new_dt = DecisionTreeRegressor(**gdt.fit(x_use, y_use).best_params_, random_state=108)\n",
    "    \n",
    "    new_rf = RandomForestRegressor(**grf.fit(x_use, y_use).best_params_, random_state=108)\n",
    "    \n",
    "    new_gb = GradientBoostingRegressor(**ggb.fit(x_use, y_use).best_params_, random_state=108)\n",
    "    \n",
    "    new_cb = CatBoostRegressor(**gcb.fit(x_use, y_use).best_params_, random_state=108, verbose=False)\n",
    "\n",
    "\n",
    "    for algo in [[new_dt, 'dt'], [new_rf, 'rf'], [new_gb, 'gb'], [new_cb, 'cb']]:\n",
    "        algo[0].fit(x_use, y_use)\n",
    "        current = np.inf\n",
    "        num = x_train_imputed.shape[1]\n",
    "        used_feature = list(x_use)\n",
    "        usee = pd.DataFrame({'params':x_use.columns, 'importances':algo[0].feature_importances_}).sort_values('importances', ascending=False)\n",
    "        for kbest in [5, 10, 15, 25, 50]:\n",
    "            uses = usee.head(kbest)['params']\n",
    "            \n",
    "\n",
    "            x_tr_try= x_use[uses]\n",
    "            \n",
    "            hold = np.mean(-cross_val_score(estimator=algo[0], X=x_tr_try, y=y_use, cv = 5, scoring = 'neg_root_mean_squared_error'))\n",
    "            if hold < current:\n",
    "                current = hold\n",
    "                num = kbest       \n",
    "                sampling = data_used[2]\n",
    "                used_feature = list(uses)\n",
    "            else:\n",
    "                None\n",
    "\n",
    "        x_tr_fin = x_use[usee.head(num)['params']]\n",
    "        x_te_fin = x_test_imputed[usee.head(num)['params']]\n",
    "        \n",
    "        y_pred = algo[0].fit(x_tr_fin, y_use).predict(x_te_fin)\n",
    "        store = mean_squared_error(y_test, y_pred)**0.5\n",
    "        name.append(algo[1])\n",
    "        k.append(num)\n",
    "        tr_auc.append(current)\n",
    "        te_auc.append(store)\n",
    "        method.append(sampling)\n",
    "        features.append(used_feature)\n",
    "\n",
    "result = pd.DataFrame({'algo':name, 'n_features':k, 'train_RMSE':tr_auc, 'test_RMSE':te_auc, 'method':method, 'features':features}).sort_values('test_RMSE', ascending=True)\n",
    "result.sort_values('test_RMSE', ascending=True).head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_used = result['algo'].iloc[0]\n",
    "features_used = result['features'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algo_used == 'dt':\n",
    "    do_train = new_dt\n",
    "elif algo_used == 'gb':\n",
    "    do_train = new_gb\n",
    "elif algo_used == 'rf':\n",
    "    do_train = new_rf\n",
    "elif algo_used == 'cb':\n",
    "    do_train = new_cb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare to retrain using all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we happened to already do our part in the x_con, so we will reuse x_con as our main retraining dataset.\n",
    "# Since we already do label encoding, we no longer need to label encode it again\n",
    "imputer, x_imputed = imput_fit_transform(x_con)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only the best features from train_test_split\n",
    "x_imputed_use = x_imputed[features_used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 15)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_imputed_use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.1, loss='ls',\n",
       "                          max_depth=None, max_features='auto',\n",
       "                          max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                          min_impurity_split=None, min_samples_leaf=2,\n",
       "                          min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
       "                          n_estimators=50, n_iter_no_change=None,\n",
       "                          presort='deprecated', random_state=108, subsample=1.0,\n",
       "                          tol=0.0001, validation_fraction=0.1, verbose=0,\n",
       "                          warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_train.fit(x_imputed_use, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1747.462100379974"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(do_train.predict(x_imputed_use), y)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_datapath= 'train.csv'\n",
    "pred_data = read_data(datapath, get_type(datapath)[0])\n",
    "pred_data = pred_data[list(x_con)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_obj = pred_data[categories]\n",
    "pred_data_obj.fillna('Unknown', inplace=True)\n",
    "pred_data_num = pred_data[set(pred_data) - set(categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_obj_le = le_transform(pred_data_obj, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_con = pd.concat([pred_data_num, pred_data_obj_le], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_con = pred_data_con[list(x_con)]\n",
    "pred_con_imputed = imput_transform(pred_data_con, imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data['prediction_result'] = do_train.predict(pred_con_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
